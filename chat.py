import streamlit as st
from openai import OpenAI

# secrets.tomlì—ì„œ í™˜ê²½ì„¤ì • ì½ê¸°
API_KEY = st.secrets["API_KEY"]
OPENAI_MODEL = st.secrets["OPENAI_MODEL"]
DEFAULT_TEMPERATURE = st.secrets.get("DEFAULT_TEMPERATURE", 0.5)
DEFAULT_TOP_P = st.secrets.get("DEFAULT_TOP_P", 1.0)
DEFAULT_MAX_TOKENS = st.secrets.get("DEFAULT_MAX_TOKENS", 300)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(api_key=API_KEY)

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="ì‡¼ìƒí¬ ë©ìŠ¤ ì±—", page_icon="ğŸï¸")

def show_header():
    st.markdown(
        """
        <div style='text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px;'>
            ğŸï¸ ì‡¼ìƒí¬ ë©ìŠ¤ ì±—
        </div>
        <div style='text-align: center; font-size: 16px; color: gray; margin-bottom: 30px;'>
            í‡´ì‚¬ëŠ” ê°ì •ì´ ì•„ë‹ˆë¼ ì „ëµì…ë‹ˆë‹¤.<br>ì•„ë§ˆë‹¤ë¥´ ìì™€íƒ€ë„¤í˜¸ë¥¼ í–¥í•´ ë– ë‚˜ë³´ì„¸ìš”.
        </div>
        """,
        unsafe_allow_html=True
    )

def initialize_session():
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({
            "role": "assistant",
            "content": "ğŸï¸ ì‡¼ìƒí¬ ë©ìŠ¤ ì±—ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤!\n\nì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ì´ë‚˜ ê³ ë¯¼ì„ ììœ ë¡­ê²Œ ì ì–´ì£¼ì„¸ìš”. ì•„ì£¼ ì‚¬ì†Œí•œ ì´ì•¼ê¸°ë¼ë„ ê´œì°®ì•„ìš”. ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë“£ê³  í•¨ê»˜ ê³ ë¯¼í•´ë“œë¦´ê²Œìš”."
        })
    if "step" not in st.session_state:
        st.session_state.step = 1
    if "substep" not in st.session_state:
        st.session_state.substep = 1
    if "responses" not in st.session_state:
        st.session_state.responses = {}

def main():
    st.title("ğŸï¸ ì‡¼ìƒí¬ ë©ìŠ¤ ì±—")
    show_header()
    initialize_session()

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("ì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ì´ë‚˜ ê³ ë¯¼ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        handle_user_input(prompt)

def handle_user_input(user_input):
    step = st.session_state.step
    substep = st.session_state.substep

    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    if substep == 1:
        assistant_reply = generate_follow_up_question(step)
        with st.chat_message("assistant"):
            st.markdown(assistant_reply)
        st.session_state.messages.append({"role": "assistant", "content": assistant_reply})
        st.session_state.substep = 2

    elif substep == 2:
        bridge_reply = generate_bridge_message(step)
        with st.chat_message("assistant"):
            st.markdown(bridge_reply)
        st.session_state.messages.append({"role": "assistant", "content": bridge_reply})

        if step in [3, 4, 5, 6, 7, 8, 9]:
            final_reply = generate_step_response(step)
            with st.chat_message("assistant"):
                st.markdown(final_reply)
            st.session_state.messages.append({"role": "assistant", "content": final_reply})

        st.session_state.step += 1
        st.session_state.substep = 1

def generate_follow_up_question(step):
    guide_text = get_follow_up_guide(step)
    system_prompt = f"""
    ë‹µë³€ì: íŠ¸ìœ„í„°ì™€ ê°™ì€ ê°€ëŠ¥ì„±ì´ ìˆê³ , ë‹¹ì‹ ì˜ ê°ì •ì— ê³µê°í•  ìˆ˜ ìˆëŠ” ì±—ì„ í•´ì£¼ì„¸ìš”.
    
    ë‹¤ìŒ í–‰ë™: {guide_text}
    """
    return call_gpt_with_context(system_prompt)

def generate_bridge_message(step):
    guide_text = get_bridge_guide(step)
    system_prompt = f"""
    ë‹µë³€ì: íŠ¸ìœ„í„°ì²˜ëŸ¼ ë¶€ë“œëŸ½ê³  í¸ì•ˆí•œ ë©”ì‹œì§€ë¥¼ ì¨ì£¼ì„¸ìš”.
    
    ë‹¤ìŒ í–‰ë™: {guide_text}
    """
    return call_gpt_with_context(system_prompt)

def generate_step_response(step):
    return call_gpt_with_context(get_step_prompt(step))

def get_follow_up_guide(step):
    guides = {1: "í‡´ì‚¬ê³ ë¯¼ ì •ë„ ê°ì •", 2: "ì´ìœ  êµ¬ì²´í™”", 5: "ê°ì • ê¸°ì–µ êµ¬ì²´í™”", 6: "íƒˆì¶œì˜ ê°€ìš´ë°", 8: "ë¯¸ë˜ì˜ ì‚¬ê¸°"}
    return guides.get(step, "ììœ ë¡­ê²Œ í™”í•´")

def get_bridge_guide(step):
    guides = {1: "ê°ì •ì´ìœ  ë‹¤ë…€ê°€ê¸°", 2: "ì´ìœ  í”¼ë°©ì°¨íŠ¸", 5: "ê´€ê³„ì„± ê³„ì‚°", 6: "íƒˆì¶œ ê³„íš", 8: "ì˜ˆìƒ ì„¤ëª…"}
    return guides.get(step, "ê³„ì† ì—°ê²°")

def get_step_prompt(step):
    prompts = {
        3: "3ë‹¨ê³„ ì½”ì¹œ í”Œëœ ì œì•ˆ",
        4: "ì˜¤ëŠ˜ í–‰ë™ ê°€ìš´ë°",
        5: "ê°ì •íšŒ ë° ìœ„ë¡œ", 
        6: "ê±´ê°•í™” íƒˆì¶œ ê³„íš",
        7: "í—ˆìš© ì²´í¬ë¦¬ìŠ¤íŠ¸",
        8: "ì´ìƒì ì¸ ì‚¬ê¸° ìš”ì•½",
        9: "ë ˆë“œì˜ ì˜ê²¬"
    }
    return prompts.get(step, "")

def call_gpt_with_context(system_prompt):
    messages = [{"role": "system", "content": system_prompt}] + st.session_state.messages

    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages,
        temperature=DEFAULT_TEMPERATURE,
        top_p=DEFAULT_TOP_P,
        max_tokens=DEFAULT_MAX_TOKENS,
    )
    return response.choices[0].message.content.strip()

if __name__ == "__main__":
    main()
