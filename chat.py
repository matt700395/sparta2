# chat.py

import streamlit as st
from openai import OpenAI
from components.header import show_header

# secrets.tomlì—ì„œ í™˜ê²½ì„¤ì • ì½ê¸°
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
OPENAI_MODEL = st.secrets["OPENAI_MODEL"]
DEFAULT_TEMPERATURE = st.secrets.get("DEFAULT_TEMPERATURE", 0.5)
DEFAULT_TOP_P = st.secrets.get("DEFAULT_TOP_P", 1.0)
DEFAULT_MAX_TOKENS = st.secrets.get("DEFAULT_MAX_TOKENS", 300)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(api_key=OPENAI_API_KEY)

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="ì‡¼ìƒí¬ ë©ìŠ¤ ì±—", page_icon="ğŸï¸")

def initialize_session():
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # ê³ ì • ì¸ì‚¿ë§ ì¶”ê°€
        st.session_state.messages.append({
            "role": "assistant",
            "content": "ğŸï¸ ì‡¼ìƒí¬ ë©ìŠ¤ ì±—ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤!\n\nì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ì´ë‚˜ ê³ ë¯¼ì„ ììœ ë¡­ê²Œ ì ì–´ì£¼ì„¸ìš”. ì•„ì£¼ ì‚¬ì†Œí•œ ì´ì•¼ê¸°ë¼ë„ ê´œì°®ì•„ìš”. ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë“£ê³  í•¨ê»˜ ê³ ë¯¼í•´ë“œë¦´ê²Œìš”."
        })
    if "step" not in st.session_state:
        st.session_state.step = 1
    if "substep" not in st.session_state:
        st.session_state.substep = 1
    if "responses" not in st.session_state:
        st.session_state.responses = {}

def main():
    st.title("ğŸï¸ ì‡¼ìƒí¬ ë©ìŠ¤ ì±—")
    show_header()
    initialize_session()

    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    if prompt := st.chat_input("ì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ì´ë‚˜ ê³ ë¯¼ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        handle_user_input(prompt)

def handle_user_input(user_input):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì¦‰ì‹œ ì¶œë ¥í•˜ê³ , ì´ì–´ì„œ GPTê°€ ì§ˆë¬¸ ìƒì„±"""
    step = st.session_state.step
    substep = st.session_state.substep

    # 1. ì‚¬ìš©ì ì…ë ¥ ì¶œë ¥
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # 2. GPT í˜¸ì¶œë¡œ ì—ì´ì „íŠ¸ ë‹µë³€ ìƒì„±
    if substep == 1:
        assistant_reply = generate_follow_up_question(step)
        with st.chat_message("assistant"):
            st.markdown(assistant_reply)
        st.session_state.messages.append({"role": "assistant", "content": assistant_reply})
        st.session_state.substep = 2

    elif substep == 2:
        bridge_reply = generate_bridge_message(step)
        with st.chat_message("assistant"):
            st.markdown(bridge_reply)
        st.session_state.messages.append({"role": "assistant", "content": bridge_reply})

        if step in [3, 4, 5, 6, 7, 8, 9]:
            final_reply = generate_step_response(step)
            with st.chat_message("assistant"):
                st.markdown(final_reply)
            st.session_state.messages.append({"role": "assistant", "content": final_reply})

        st.session_state.step += 1
        st.session_state.substep = 1

def generate_follow_up_question(step):
    """GPTë¥¼ í˜¸ì¶œí•´ì„œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±"""
    guide_text = get_follow_up_guide(step)
    system_prompt = f"""
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ëŒ€í™” ì½”ì¹˜ì…ë‹ˆë‹¤.
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì¡°ê¸ˆ ë” ìì„¸íˆ ì´ì•¼ê¸°í•  ìˆ˜ ìˆë„ë¡ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ì¶”ê°€ ì§ˆë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¶”ê°€ ì§ˆë¬¸ì˜ ëª©í‘œ:
{guide_text}
"""
    return call_gpt_with_context(system_prompt)

def generate_bridge_message(step):
    """GPTë¥¼ í˜¸ì¶œí•´ì„œ ë‹¤ë¦¬ ë©˜íŠ¸ ìƒì„±"""
    guide_text = get_bridge_guide(step)
    system_prompt = f"""
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ëŒ€í™” ì½”ì¹˜ì…ë‹ˆë‹¤.
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ë‹µë³€ì„ ê³µê°í•´ì¤€ í›„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°€ëŠ” ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ ê°€ì´ë“œ:
{guide_text}
"""
    return call_gpt_with_context(system_prompt)

def generate_step_response(step):
    """GPTë¥¼ í˜¸ì¶œí•´ì„œ ê° ìŠ¤í…ë³„ ì£¼ìš” ë‹µë³€ ìƒì„± (ëŒ€í™” ë‚´ì—­ ë°˜ì˜)"""
    if step == 3:
        system_prompt = """
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê¸ì •ì ì¸ ëŒ€í™” ì½”ì¹˜ì…ë‹ˆë‹¤.
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ í‡´ì‚¬ ê³ ë¯¼ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ 3ë‹¨ê³„ ì½”ì¹­ í”Œëœì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. ì˜¤ëŠ˜ í•  ìˆ˜ ìˆëŠ” ì•„ì£¼ ì‘ì€ í–‰ë™
2. ì´ë²ˆ ì£¼ ì•ˆì— ì„¸ìš¸ ìˆ˜ ìˆëŠ” ì‘ì€ ëª©í‘œ
3. ì¥ê¸°ì ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ì¸ìƒ ë°©í–¥

ì§„ì‹¬ ì–´ë¦° ê³µê°ê³¼ í¬ë§ì„ ë‹´ì•„ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    elif step == 4:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì˜¤ëŠ˜ í•˜ë£¨ ì•ˆì— ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ì‘ê³  ë¶€ë‹´ ì—†ëŠ” ì¤€ë¹„ í–‰ë™ 1ê°€ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
"""
    elif step == 5:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬,

1. ì‚¬ìš©ìì˜ ê°ì •ì„ ë¶€ë“œëŸ½ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ ,
2. ì´ì–´ì„œ ë”°ëœ»í•œ ìœ„ë¡œì˜ ë¬¸ì¥ì„ í•˜ë‚˜ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    elif step == 6:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì„ íƒí•œ íƒˆì¶œ ê²½ë¡œ(ì´ì§, íœ´ì‹ ë“±)ë¥¼ 3ê°œì›”, 6ê°œì›” ì•ˆì— í˜„ì‹¤í™”í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì¸ í”Œëœì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""
    elif step == 7:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì„ íƒí•œ íƒˆì¶œ ê²½ë¡œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì¤€ë¹„ë¬¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ 5ê°œ í•­ëª©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    elif step == 8:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ê¿ˆê¾¸ëŠ” ì´ìƒì ì¸ ì‚¶ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ì•„ë¦„ë‹µê³  í¬ë§ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
    elif step == 9:
        system_prompt = """
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, í‡´ì‚¬ë¥¼ ê³ ë¯¼í•˜ëŠ” ì‚¬ìš©ìì—ê²Œ ì˜í™” 'ì‡¼ìƒí¬ íƒˆì¶œ' ì† ë ˆë“œê°€ í•  ë²•í•œ ì§§ê³  í¬ë§ì ì¸ ì¡°ì–¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    else:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤."

    return call_gpt_with_context(system_prompt)

def get_follow_up_guide(step):
    """ê° ìŠ¤í…ë³„ ì¶”ê°€ ì§ˆë¬¸ ê°€ì´ë“œ"""
    guides = {
        1: "í‡´ì‚¬ë¥¼ ì–¼ë§ˆë‚˜ ê³ ë¯¼í•˜ê³  ìˆëŠ”ì§€ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.",
        2: "í‡´ì‚¬ë¥¼ ê³ ë¯¼í•˜ëŠ” ì´ìœ ë¥¼ êµ¬ì²´ì ì¸ ê²½í—˜ì´ë‚˜ ìƒí™©ìœ¼ë¡œ í’€ì–´ë‚¼ ìˆ˜ ìˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.",
        5: "ê°ì •ì˜ ê¹Šì´ì™€ ì›ì¸ì„ ë” ê¹Šê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸í•©ë‹ˆë‹¤.",
        6: "ì„ íƒí•œ íƒˆì¶œ ê²½ë¡œì— ëŒ€í•œ ê°œì¸ì ì¸ ë™ê¸°ë¥¼ ë” ê¹Šì´ íŒŒì•…í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.",
        8: "ê¿ˆê¾¸ëŠ” ì´ìƒì ì¸ ì‚¶ì— ëŒ€í•´ ë” êµ¬ì²´ì ì¸ ìƒìƒì„ ëŒì–´ëƒ…ë‹ˆë‹¤.",
    }
    return guides.get(step, "ì‚¬ìš©ìê°€ ìì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ì¡°ê¸ˆ ë” ìì„¸íˆ í’€ì–´ë‚´ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.")

def get_bridge_guide(step):
    """ê° ìŠ¤í…ë³„ ë‹¤ë¦¬ ë©˜íŠ¸ ê°€ì´ë“œ"""
    guides = {
        1: "ê°ì • ì˜¨ë„ íŒŒì•… í›„, í‡´ì‚¬ ê³ ë¯¼ ì´ìœ ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
        2: "í‡´ì‚¬ ê³ ë¯¼ ì´ìœ  íŒŒì•… í›„, ì½”ì¹­ í”Œëœ ì œì•ˆìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
        5: "ê°ì •ì„ ì´í•´í•œ í›„, ë‹¤ìŒ ì‹¤ì§ˆì  ë‹¨ê³„ë¡œ ë¶€ë“œëŸ½ê²Œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
        6: "íƒˆì¶œ ê²½ë¡œë¥¼ ë“¤ì€ í›„, ì¤€ë¹„ë¬¼ ì²´í¬ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°‘ë‹ˆë‹¤.",
        8: "ê¿ˆê¾¸ëŠ” ì‚¶ì„ ë“¤ì€ í›„, ë§ˆì§€ë§‰ ì¡°ì–¸ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.",
    }
    return guides.get(step, "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì¸ì •í•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°‘ë‹ˆë‹¤.")

def call_gpt_with_context(system_prompt):
    """OpenAIì— system prompt + ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ë‚´ ë‹µë³€ ìƒì„±"""
    messages = [{"role": "system", "content": system_prompt}] + st.session_state.messages

    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages,
        temperature=DEFAULT_TEMPERATURE,
        top_p=DEFAULT_TOP_P,
        max_tokens=DEFAULT_MAX_TOKENS,
    )
    return response.choices[0].message.content.strip()

if __name__ == "__main__":
    main()
